{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYI7qXp2Xg8H"
   },
   "source": [
    "This notebook is an assignement given by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) as part of the [Machine Learning class](https://github.com/erachelson/MLclass).\n",
    "\n",
    "This notebook is based on [Andrew NG courses](https://www.coursera.org/learn/machine-learning), on [Andrew NG book](https://www.deeplearning.ai/content/uploads/2018/09/Ng-MLY01-12.pdf), on https://www.youtube.com/watch?v=F1ka6a13S9I&feature=youtu.be and on http://scott.fortmann-roe.com/docs/BiasVariance.html.\n",
    "\n",
    "This notebook was written by [Robin Escallier](https://github.com/rescallier)\n",
    "##### First let's import some prerequisites, while you'll read the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcXOw23rXg8c"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from IPython.display import Image\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2n2S0RmxXg8k"
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">The bias - variance tradeoff</div>\n",
    "\n",
    "We are going here to analyze error while doing machine learning predictions on a test set. To do so, we will split the error between bias and variance. To decrease the error, increasing the size of training data is not always a good solution, we will have different approach whether spotting bias or variance as main error.\n",
    "1. [Spliting the error](#sec1)\n",
    "    1. [Conceptual Definition](#sec1-1)\n",
    "    2. [Mathematical Definition](#sec1-2)\n",
    "2. [Spoting Bias or Variance](#sec2)\n",
    "3. [Model Size Impact on Bias and Variance](#sec3)\n",
    "4. [Other Techniques to Deal with Bias and Variance](#sec4)\n",
    "    1. [Regularization](#sec4-1)\n",
    "    2. [Early Stopping](#sec4-2)\n",
    "\n",
    "<div class=\"alert alert-success\"><b>In a nutshell:</b>\n",
    "<ul>\n",
    "<li> Test set Errors can be split in Variance and Bias\n",
    "<li> Variance is due to an overfitting of the training set\n",
    "<li> Bias is spotted with the training set error\n",
    "<li> Variance is spotted using the comparison between training error and test error \n",
    "<li> While decreasing Variance, there is a risk of increasing Bias\n",
    "<li> While decreasing Bias, there is a risk of increasing Variance   \n",
    "<li> The best way to adress Bias is to increase model size\n",
    "<li> The best way to adress Variance is to get new training data\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LChLlHJJXg8n"
   },
   "source": [
    "# 1. <a id=\"sec1\"></a>Spliting the error\n",
    "Suppose that we try to predict y according to X, we are going to define bias and variance in two ways :\n",
    "\n",
    "## <a id=\"sec1-1\"></a> 1.A Conceptual Definition\n",
    "\n",
    "- The error due to bias is the difference between the expected (or average) prediction of our model and the true value of y. Imagine you train different model using different set of your full dataset. Due to randomness in the underlying data sets, the resulting models will have different predictions. Bias measures how far off in general these models' predictions are from the correct value.\n",
    "\n",
    "- The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can train your model multiple times. The variance is how much the predictions for a given point vary between different training of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    We can illustrate this using  bulls-eye diagram. The center of the target is the perfect model. And the hit are different realizations of the model.\n",
    "    \n",
    "<img src=\"img/Graphical_illustration_of_bias_and_variance.png\" width=\"800px\"></img>\n",
    "\n",
    "## <a id=\"sec1-2\"></a> 1.B Mathematical Definition\n",
    "\n",
    "We try to predict y using Y. To do so, we suppose that there is a relationship between X and y : $ y = f(X) + \\epsilon $ with the error term $\\epsilon$ is normally distributed: $ \\epsilon \\sim N(0,\\sigma_e) $\n",
    "\n",
    "We estimate a model $\\hat{f}(X)$ using a neural network or a linear regression for example.\n",
    "Using this model, the squarred error at a point x is:  $Err(x) =  E[(Y-\\hat{f}(x))²] $\n",
    "\n",
    "We can decompose this error in Bias and Variance : $Err(x)=(E[\\hat{f}(x)]−f(x))²+E[(\\hat{f}(x)−E[\\hat{f}(x)])²]+\\sigma_e$\n",
    "$Err(x)=Bias²+Variance + Irreductible$ $error$\n",
    "\n",
    "The irreductible error is a noise term that cannot fundamentally be reduced by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xo2yS3naXg8p"
   },
   "source": [
    "# 2. <a id=\"sec2\"></a>Spotting Bias or Variance\n",
    "Suppose we want to predict a variable y according to X, we split our data in traning set and in test set. So we have a training set error and a test set error. We want to have a low error on test set.\n",
    "\n",
    "- First, the algorithm’s error rate on the training set. We think of this informally as the algorithm’s bias.\n",
    "\n",
    "- Second, how much worse the algorithm does on the test set than the training set. We think of this informally as the algorithm’s variance.\n",
    "\n",
    "To illustrate this idea, we will work on the fashion MNist using a neural network: \n",
    "\n",
    "The first model that we are going to use is a simple linear model, we will look at bias and variance on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "From now and until the end of the notebook, you will be asked to try to implement models without training them, then to look at the solution that I provide and to load train models. \n",
    "The models used here are too long to train to be train in one hour.\n",
    "\n",
    "If tou want to, you can uncomment lines with training and running the full notebook but try to do it on a gpu and it will take more than one hour.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ah9CRq8qXg8r"
   },
   "source": [
    "We download and preprocess our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define some constants\n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqYBPgp0Xg8u"
   },
   "outputs": [],
   "source": [
    "# function to preprocess the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Load both train and test set\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='../data',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='../data',\n",
    "                                             train=False, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "# Define loader to access data by batch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vS1Et8M3n44I"
   },
   "outputs": [],
   "source": [
    "def train(cnn,train_loader,num_epochs,optimizer,criterion):\n",
    "    '''\n",
    "    Train the model\n",
    "    -------\n",
    "    \n",
    "    Param:\n",
    "        cnn : torch.nn.module, model to train\n",
    "        train_loader : torch.utils.data.DataLoader, loader with the data to train the model on\n",
    "        num_epochs : int, number of epoch \n",
    "        optimizer : torch.optim, optimizer to use during the training\n",
    "        criterion: torch.nn, loss function used here\n",
    "    '''\n",
    "    losses = [];\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader): \n",
    "        \n",
    "            images = Variable(images.float())\n",
    "            labels = Variable(labels)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data);\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                  print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
    "                    %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7Xre2Ewn44N"
   },
   "outputs": [],
   "source": [
    "# A function to compute accuracy of our model on a loader\n",
    "def accuracy(cnn,loader,kind_of_set):\n",
    "    '''\n",
    "    Compute and print accuracy of the model on a dataset\n",
    "    --------\n",
    "    Param : \n",
    "        cnn : torch.nn.module, model used\n",
    "        loader : torch.utils.data.DataLoader, loader with the data to compute the accuracy on\n",
    "        kind_of_set: str, 'train' or 'test' just to print the kind of set working on \n",
    "    -------\n",
    "    \n",
    "    Return :\n",
    "        correct : float, number of good predictions\n",
    "        total: float, number of predictions\n",
    "    '''\n",
    "    cnn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images = Variable(images.float())\n",
    "        outputs = cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Accuracy of the model on the ' + str(total)+ ' '+ kind_of_set+\n",
    "          ' images: '+ str(100 * correct.detach().numpy() / total))\n",
    "    return correct.detach().numpy()/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**<br>\n",
    "Implement a class wich inherit from nn.module, this class has to compute a simple linear model using neural network\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Define the model used\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # To do initialize layers of the network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # To do compute forward propagation of the model\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train the model for one epoch to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn_linear = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_linear.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train(cnn_linear,train_loader,1,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the solution to load a model already train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9tq5oWKXg81"
   },
   "outputs": [],
   "source": [
    "# %load solutions/linear_model.py\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Define the model \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(28*28*1, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "OTMYVtGrn44T",
    "outputId": "14f6dc09-7a8c-4fe6-fb0c-038c2c2dd4f3"
   },
   "outputs": [],
   "source": [
    "# set the seed to get the same results at each run\n",
    "torch.manual_seed(0)\n",
    "cnn_linear = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_linear.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## To train the model uncomment lines below, \n",
    "#train(cnn_linear,train_loader,num_epochs,optimizer,criterion)\n",
    "#torch.save(cnn_linear, 'models/linear_model.pt')\n",
    "\n",
    "# Load trained model that was train using the code above using a gpu on google colab during 30 epochs\n",
    "cnn_linear = torch.load('models/linear_model.pt')\n",
    "cnn_linear.eval()\n",
    "# \n",
    "\n",
    "accuracy_train_linear = accuracy(cnn_linear,train_loader,'train')\n",
    "accuracy_test_linear  = accuracy(cnn_linear,test_loader,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1t-5TLgI7K2"
   },
   "source": [
    "So here our test error is 16%, which is a high, we can separate it into bias and variance\n",
    "The bias is 13.% while the variance is equal to 4.9%. So here our algorithm have a high bias. We are going to see how to adress it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VkqpqugBXg9Z"
   },
   "source": [
    "# 3. <a id=\"sec3\"></a>Model Size Impact on Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6wn1i7lH7VJ"
   },
   "source": [
    "The first ideas to address bias and variances issues are the following :\n",
    "\n",
    "- If the bias is high( the algorithm is underfitting ), increase the size of the mode \n",
    "\n",
    "- If the variance is high( the algorithm is overfitting ), add data to the training set\n",
    "\n",
    "But we can not get infinite amount of data( for example here we can not add data to the training set), and sometimes increasing the size of the model will eventually cause computational problems.\n",
    "\n",
    "And increasing the size of the model could easily lead to overfitting, this is what is called bias and variance tradeoff. Here is a graph to illustrate it:\n",
    "<img src=\"img/biasvariance.png\" width=\"800px\"></img>\n",
    "\n",
    "\n",
    "So now we are going to see this tradeoff by increasing the size of the model and looking at the impact on both training and test error:\n",
    "\n",
    "We are going to use a big convolutionnal neural network with the following structure:\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 16 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 32 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 64 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- A linear layer with no activation ( no needed beacause using Crossentropyloss) with an output of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**<br>\n",
    "Implement a class wich inherit from nn.module, this class has to define the model describe above\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Define the model used\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # To do initialize layers of the network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # To do compute forward propagation of the model\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train the model for one epoch to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn_3conv = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_3conv.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train(cnn_3conv,train_loader,1,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjEKcMAJXg9a"
   },
   "outputs": [],
   "source": [
    "# %load solutions/3conv_model.py\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(3*3*64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0S6fITkSH7VS",
    "outputId": "6e1dc9d6-c320-4990-e6c8-20488b1a29a2"
   },
   "outputs": [],
   "source": [
    "# set the seed to get the same results at each run\n",
    "torch.manual_seed(0)\n",
    "cnn_3conv = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_3conv.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## To train the model uncomment lines below, \n",
    "#train(cnn_3conv,train_loader,num_epochs,optimizer,criterion)\n",
    "#torch.save(cnn_3conv, 'models/3steps_models.pt')\n",
    "\n",
    "# Load trained model that was train using the code above using a gpu on google colab during 30 epochs\n",
    "cnn_3conv = torch.load('models/3steps_models.pt')\n",
    "cnn_3conv.eval()\n",
    "accuracy_train_3conv = accuracy(cnn_3conv,train_loader,'train')\n",
    "accuracy_test_3conv  = accuracy(cnn_3conv,test_loader,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here our test error is 9.3%, which is a bit high, we can separate it into bias and variance\n",
    "The bias is 0.7% while the variance is equal to 8.6%. So here our algorithm have a high variance. We achieve to decrease the bias of the alorithm but this makes the variance increase because of the size of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the size of the model\n",
    "So here we have a very high variance wich is the main component of our error. So to adress it, we are going to decrease the size of the model. \n",
    "\n",
    "We are going to use a smaller convolutionnal neural network with the following structure:\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 16 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 32 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- A linear layer with no activation ( no needed beacause using Crossentropyloss) with an output of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**<br>\n",
    "Implement a class wich inherit from nn.module, this class has to define the model describe above\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Define the model used\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # To do initialize layers of the network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # To do compute forward propagation of the model\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train the model for one epoch to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn_2conv = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_2conv.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train(cnn_2conv,train_loader,1,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/2conv_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the seed to get the same results at each run\n",
    "torch.manual_seed(0)\n",
    "cnn_2conv = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_2conv.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## To train the model uncomment lines below, \n",
    "#train(cnn_2conv,train_loader,num_epochs,optimizer,criterion)\n",
    "#torch.save(cnn_2conv, 'models/2steps_model.pt')\n",
    "\n",
    "# Load trained model that was train using the code above using a gpu on google colab during 30 epochs\n",
    "cnn_2conv = torch.load('models/2steps_model.pt')\n",
    "cnn_2conv.eval()\n",
    "accuracy_train_2conv = accuracy(cnn_2conv,train_loader,'train')\n",
    "accuracy_test_2conv  = accuracy(cnn_2conv,test_loader,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here our test error is 9.6%, which is a bit high, we can separate it into bias and variance\n",
    "The bias is 1.4% while the variance is equal to 8.2%. So here our algorithm have still a high variance. We achieve to decrease a bit the variance of the alorithm but we did not achieve to decrease the overall error because of the increase of bias due to decrease of model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the size of the model ( again)\n",
    "So here we still have a very high variance wich is the main component of our error. So to adress it, we are going to decrease the size of the model. \n",
    "\n",
    "We are going to use a smaller convolutionnal neural network with the following structure:\n",
    "- Convolutional layer with a kernel of 5, a padding of 2 and 16 outputs channels\n",
    "- Batchnormalization\n",
    "- Relu\n",
    "- Maxpooling by blocks of size 2×2\n",
    "\n",
    "- A linear layer with no activation ( no needed beacause using Crossentropyloss) with an output of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %load solutions/1conv_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to get the same results at each run\n",
    "torch.manual_seed(0)\n",
    "cnn_1conv = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_1conv.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## To train the model uncomment lines below, \n",
    "#train(cnn_1conv,train_loader,num_epochs,optimizer,criterion)\n",
    "#torch.save(cnn_1conv, 'models/1step_model.pt')\n",
    "\n",
    "# Load trained model that was train using the code above using a gpu on google colab during 30 epochs\n",
    "cnn_1conv = torch.load('models/1step_model.pt')\n",
    "cnn_1conv.eval()\n",
    "accuracy_train_1conv = accuracy(cnn_1conv,train_loader,'train')\n",
    "accuracy_test_1conv  = accuracy(cnn_1conv,test_loader,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here our test error is 9.8%, which is a bit high, we can separate it into bias and variance\n",
    "The bias is 3.9% while the variance is equal to 5.9%. So here our algorithm have still a high variance.\n",
    "\n",
    "We achieve to decrease a bit the variance of the alorithm but we did not achieve to decrease the overall error because of the increase of bias due to decrease of model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on the size of the model\n",
    "By increasing the size of our model, we improve our overall error but the variance increased a lot. So we need to make it decrease but we have seen that reducing a bit the size of our model does not improve the overall error despite the variance decrease because of the increase of the bias.\n",
    "\n",
    "##### This is what is called bias and variance tradeoff :  there are some machine learning algorithm tuning that reduce bias errors but at the cost of increasing variance, and vice versa. We have seen here that increasing the size of the model—adding neurons/layers in a neural network— reduces bias but could increase variance.\n",
    "\n",
    "We are now going to see other ways of dealing with bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id=\"sec4\"></a>Other Techniques to Deal with Bias and Variance\n",
    "\n",
    "\n",
    "### Dealing with bias\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    " To deal with bias we can :\n",
    "<li> - Increase model size\n",
    "<li> - Modify input features based on insights from error analysis(looking at your error deeplu using informations about mispredicted samples), it will not be done here because we only have images as inputs\n",
    "<li> - Reduce or eliminate regularization, it will not be done here because we don't have any regulariztion for now\n",
    "<li> - Modify model architecture, that's what we have done going from linear model to convolutionnal neural networks\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Dealing with variance\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "To deal with variance, we can:\n",
    "\n",
    "<li>- Add more training data, we don't have more here\n",
    "<li>- Add regularization(L2 regularization, L1 regularization, dropout): It reduces variance but increases bias.We are going to try to implement it\n",
    "<li>- Add early stopping(stop gradient descent early, using validation error):  It reduces variance but increases bias.We are going to try to implement it\n",
    "<li>- Feature selection to decrease number/type of input features: I works well while working on small dataset, it can increase bias\n",
    "<li>- Decrease the model size: we have seen it but it increases bias.\n",
    "<li>- Modify model architecture: It can also help to decrease variance\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "So here our bias error of the model with 3 convolutionnals layers seems good, so we are going to try to decrease variance using 2 techniques : regularization and dropouts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec4-1\"></a> 4.A Regularization\n",
    "So because our varianceof the model with 3 convolutions is very high, we will try to make it decreased using regularization, we are going to use L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZEGYGHlXg9h"
   },
   "outputs": [],
   "source": [
    " %load solutions/3conv_model.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the optimizer to add L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "#regularization parameter lambda\n",
    "lambda_ = 0.005\n",
    "cnn_regularization = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# To do set the optimizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(cnn_regularization,train_loader,1,optimizer,criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %load solutions/regularization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here our test error is 8.7%, which is a bit better, we can separate it into bias and variance\n",
    "The bias is 5.8% while the variance is equal to 2.9%. We achieve to decrease the variance at the cost of increasing our bias because of the bias-variance tradeoff but the overall error is slightly better.\n",
    "\n",
    "Let's see if we can have better results using early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec4-2\"></a> 4.B Early Stopping\n",
    "Let's try to implement early stopping and see the results on both bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to define a validation loader and to define an early stopping class to use to compare validation loss at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "trainset, full_validset = torch.utils.data.random_split(full_trainset, (10000, 50000))\n",
    "validset, _ = torch.utils.data.random_split(full_validset, (1000, 49000))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def validation(cnn,validloader,criterion):\n",
    "    '''\n",
    "    compute the loss on a validset\n",
    "    -------\n",
    "    \n",
    "     Param:\n",
    "        cnn : torch.nn.module, model to evaluate\n",
    "        validloader : torch.utils.data.DataLoader, loader with the data to valid the model on\n",
    "        criterion: torch.nn, loss function used here\n",
    "    -------\n",
    "    \n",
    "    Return:\n",
    "        valid_loss: loss on the validation set\n",
    "    '''\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    return valid_loss\n",
    "\n",
    "# We define an early stopping class\n",
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.delta = delta\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: %d / %d' % (self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**<br>\n",
    "Change the train function to add early stopping\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define new train function using validation set and early stopping\n",
    "def train(cnn,train_loader,num_epochs,optimizer,criterion,validloader,patience):\n",
    "    '''\n",
    "    Train the model\n",
    "    -------\n",
    "    \n",
    "    Param:\n",
    "        cnn : torch.nn.module, model to train\n",
    "        train_loader : torch.utils.data.DataLoader, loader with the data to train the model on\n",
    "        num_epochs : int, number of epoch \n",
    "        optimizer : torch.optim, optimizer to use during the training\n",
    "        criterion: torch.nn, loss function used here,\n",
    "        validloader: torch.utils.data.DataLoader, loader with the data to validate the model on\n",
    "        patience: int, number of epoch to wait with an higher loss before stopping the algorithm\n",
    "    '''\n",
    "    losses = []\n",
    "    validlosses = []\n",
    "    estop = EarlyStopping(patience=patience)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader): \n",
    "        \n",
    "            images = Variable(images.float())\n",
    "            labels = Variable(labels)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.data);\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                  print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
    "                    %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.detach().numpy()))\n",
    "       # To do : compute valid loss and break the for loop if necessary\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/earlystopping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/3conv_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=3\n",
    "torch.manual_seed(0)\n",
    "\n",
    "cnn_early_stopping = CNN()\n",
    "# CrossEntropyLoss as loss because of no softmax in the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_early_stopping.parameters(),\n",
    "                             lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## To train the model uncomment lines below, \n",
    "#train(cnn_early_stopping,train_loader,num_epochs,optimizer,criterion,validloader,patience)\n",
    "#torch.save(cnn_early_stopping, 'models/early_stopping.pt')\n",
    "\n",
    "# Load trained model that was train using the code above using a gpu on google colab during 30 epochs\n",
    "\n",
    "cnn_early_stopping = torch.load('models/early_stopping.pt')\n",
    "cnn_early_stopping.eval()\n",
    "accuracy_train_early_stopping = accuracy(cnn_early_stopping,train_loader,'train')\n",
    "accuracy_test_early_stopping  = accuracy(cnn_early_stopping,test_loader,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training stopped after 23 epochs\n",
    " \n",
    "So here our test error is 8.9%, which is a bit better, we can separate it into bias and variance The bias is 0.6% while the variance is equal to 8.3%. We achieve to decrease the variance a bit without increasing to much bias, the overall error is slightly better than the model without early stopping. But the lowest test error is reached using regularization"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tradeoff_bias_variance_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
